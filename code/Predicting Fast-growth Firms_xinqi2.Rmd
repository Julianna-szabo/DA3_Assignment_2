---
title: "`r params$dynamictitle`"
author: "Julianna Szabo & Xinqi Wang"
params:
  dynamictitle: 'Assignment 2: Predicting Fast-growth Firms'
  viridis_palette: viridis
output:
  pdf_document: 
  html_document:
    code_download: yes
    highlighter: null
    theme: flatly
  code_download: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Introduction: 
In this study, we are interested in predicting fast-growth firms. In reality, only a small fraction of firms achieve the highest rate of growth. Most firms either grow fast or die slow. Therefore, it is crucial to predict that fast-growth firms now will continue to be fast-growing in the future. We will use firms' financial and attributes data in a European country from the binsnode-firms dataset. We will also build and compare different predictive models and classify them into prospective fast-growth firms and prospective non-fast-growth firms to achieve the goal. 

  
## 2. Data and Sample Design: 
The original dataset is a large panel that covers the entire period of 2005 to 2016. We first filter it down to 2010 â€“ 2015 and focus on a cross-section of firms only in 2012. Our question would be whether they keep fast-growing in the subsequent year. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Clear environment
rm(list = ls())

# Load libraries
library(tidyverse)
library(haven)
library(glmnet)
library(grid)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)

library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(partykit)
library(rpart.plot)
library(gridExtra)

# import data

# data_url <- "https://raw.githubusercontent.com/Julianna-szabo/DA3_Assignment_2/main/data/cs_bisnode_panel.csv?token=AREBRMDA52YMHJHLWGZLSZDADU7K4"
# data <- read_csv(file = data_url)

data <- read_csv('../Assignment_2/Data/clean/cs_bisnode_panel.csv')

data_original <- data

# drop variables with many NAs
data <- data %>%
  select(-c(COGS, finished_prod, net_dom_sales, net_exp_sales, wages)) %>%
  filter(year !=2016)

options(digits = 3)
###########################################################
# label engineering
###########################################################
# add all missing year and comp_id combinations -
# originally missing combinations will have NAs in all other columns
# data <- data %>%
#   complete(year, comp_id)

# Filter 2010 - 2015

data <- data %>% 
  filter(year >= 2011 & year <=2015)

# Sort by company id

data <- data[order(data$comp_id),]

# Size and growth
summary(data$sales) # There will be NAs, we'll drop them soon

data <- data %>%
  mutate(sales = ifelse(sales < 0, 1, sales),
         ln_sales = ifelse(sales > 0, log(sales), 0),
         sales_mil=sales/1000000,
         sales_mil_log = ifelse(sales > 0, log(sales_mil), 0))

data <- data %>%
  group_by(comp_id) %>%
  mutate(d1_sales_mil_log = sales_mil_log - Lag(sales_mil_log, 1) ) %>%
  ungroup()


# replace w 0 for new firms + add dummy to capture it
data <- data %>%
  mutate(age = (year - founded_year) %>%
           ifelse(. < 0, 0, .),
         new = as.numeric(age <= 1) %>% #  (age could be 0,1 )
           ifelse(balsheet_notfullyear == 1, 1, .),
         d1_sales_mil_log = ifelse(new == 1, 0, d1_sales_mil_log),
         new = ifelse(is.na(d1_sales_mil_log), 1, new),
         d1_sales_mil_log = ifelse(is.na(d1_sales_mil_log), 0, d1_sales_mil_log))


############################
# Find fast growing firms  #
############################

# Filter for only years 2012 and 2013

data <- data %>% 
  filter(year == 2012 | year == 2013)

# Check for missing values

count_missing_values <- function(data) {
  num_missing_values <- map_int(data, function(x) sum(is.na(x)))
  num_missing_values[num_missing_values > 0]
}
count_missing_values(data)

# Drop 

data<- filter(data, !is.na(profit_loss_year)) 
data<- filter(data, !is.na(sales)) 
data<- filter(data, !is.na(inc_bef_tax)) 


# Drop company IDs which do not have observations for both years
comp_count_check <- data %>% 
  group_by(comp_id) %>% 
  count()

missing <- comp_count_check %>% 
  filter(n<2)

data <- data[!(data$comp_id %in% (missing$comp_id)),]

rm(missing, comp_count_check)

# Create year on year column

# Columns to do year on year for
y_on_y <- c("profit_loss_year", "sales", "inc_bef_tax")

# Look at all unique values for com_id
for (i in unique(data$comp_id)) {
  # Look at all the columns to use for year on year
  for (j in y_on_y) {
    column_name <- (paste0(j, "_y_on_y"))
    # Find the row numbers
    row_2013 <- which(data$comp_id == i & data$year == 2013)
    row_2012 <- which(data$comp_id == i & data$year == 2012)
    # Create the year on year columns
    data[row_2013, column_name] <- (data[row_2013, j] - data[row_2012, j]) / data[row_2012, j] 
  }
}

#transform the data, take the 2013 rows and make them into columns, so we only left with 2012 explantory varibales
new_data_2013<-filter(data, year==2013)
new_data_2012<-filter(data, year==2012)

data <- as.data.frame(cbind(new_data_2012, new_data_2013$profit_loss_year_y_on_y, new_data_2013$sales_y_on_y, new_data_2013$inc_bef_tax_y_on_y))

data <- mutate(data, profit_loss_year_y_on_y = new_data_2013$profit_loss_year_y_on_y,
               sales_y_on_y = new_data_2013$sales_y_on_y,
               inc_bef_tax_y_on_y = new_data_2013$inc_bef_tax_y_on_y)

drops <- c('new_data_2013$profit_loss_year_y_on_y','new_data_2013$sales_y_on_y','new_data_2013$inc_bef_tax_y_on_y')

data <- data[ , !(names(data) %in% drops)]

# save it into clean file 
write.csv(data, "Data/fast_growth_workingfile_1.csv")

# data <- read_csv(file = "data/fast_growth_workingfile_1.csv")

##########################################################################
# Check the three different possible variables: sales, profit and EBIDTA

# Filter out infinates
data<- filter(data, !is.infinite(profit_loss_year_y_on_y)) 
data<- filter(data, !is.infinite(sales_y_on_y)) 
data<- filter(data, !is.infinite(inc_bef_tax_y_on_y)) 

# Filter out NAs 
data<- filter(data, !is.na(profit_loss_year_y_on_y)) 
data<- filter(data, !is.na(sales_y_on_y)) 
data<- filter(data, !is.na(inc_bef_tax_y_on_y))

glimpse(data)

##############################
# 1). Growth on sales_y_on_y:
##############################
# check growth on sales statistics
data %>% 
  summarise(
    Obs = 'sales_y_on_y',
    n = sum( !is.na( sales_y_on_y ) ),
    mean = mean(sales_y_on_y),
    first_quartile = quantile( !is.na(sales_y_on_y), 0.25),
    median   = median(sales_y_on_y),
    third_quartile = quantile(!is.na(sales_y_on_y), 0.75),
    min = min(sales_y_on_y),
    max = max(sales_y_on_y),
    sd = sd(sales_y_on_y)
  )
# from the summary statistics, we should filter out extreme variables. Let's restrict max growth on sales at 200%, and min growth
# (max loss) at -200%

# check summary statistics again:
summary_sales_growth <- data %>% 
  filter(sales_y_on_y < 2 & sales_y_on_y > -2) %>% 
  summarise(
    Obs = 'sales_y_on_y',
    n = sum( !is.na(sales_y_on_y )),
    mean = mean(sales_y_on_y),
    first_quartile = quantile(sales_y_on_y, 0.25),
    median   = median(sales_y_on_y),
    third_quartile = quantile(sales_y_on_y, 0.75),
    min = min(sales_y_on_y),
    max = max(sales_y_on_y),
    sd = sd(sales_y_on_y)
  )

count(data, sales_y_on_y > 0)

# check distribution
data %>% 
  filter(sales_y_on_y < 2 & sales_y_on_y > -2) %>% 
  ggplot(aes(x = sales_y_on_y)) +
  geom_histogram(bins = 100)


###########################################
# 2). Growth on profit_loss_year_y_on_y:
###########################################
# Distribution of profit year on year
data %>% 
  summarise(
    Obs = 'profit_loss_year_y_on_y',
    n = sum( !is.na( profit_loss_year_y_on_y ) ),
    mean = mean(profit_loss_year_y_on_y),
    first_quartile = quantile(profit_loss_year_y_on_y, 0.25),
    median   = median(profit_loss_year_y_on_y),
    third_quartile = quantile(profit_loss_year_y_on_y, 0.75),
    min = min(profit_loss_year_y_on_y),
    max = max(profit_loss_year_y_on_y),
    sd = sd(profit_loss_year_y_on_y)
  )
# from the summary statistics, we should filter out extreme variables. Let's restrict max growth on profit at 200%, and min growth
# (max loss) at -200%

# check summary statistics again:
summary_profit_growth <- data %>% 
  filter(profit_loss_year_y_on_y < 2 & profit_loss_year_y_on_y > -2) %>% 
  summarise(
    Obs = 'profit_loss_year_y_on_y',
    n = sum( !is.na( profit_loss_year_y_on_y ) ),
    mean = mean(profit_loss_year_y_on_y),
    first_quartile = quantile(profit_loss_year_y_on_y, 0.25),
    median   = median(profit_loss_year_y_on_y),
    third_quartile = quantile(profit_loss_year_y_on_y, 0.75),
    min = min(profit_loss_year_y_on_y),
    max = max(profit_loss_year_y_on_y),
    sd = sd(profit_loss_year_y_on_y)
  )

count(data, profit_loss_year_y_on_y > 0)

# check distribution
data %>% 
  filter(profit_loss_year_y_on_y < 2 & profit_loss_year_y_on_y > -2) %>% 
  ggplot(aes(x = profit_loss_year_y_on_y)) +
  geom_histogram(bins = 100)


#####################################
# 3). Growth on inc_bef_tax_y_on_y:
#####################################
# Distribution of EBIDTA year on year
data %>% 
  summarise(
    Obs = 'inc_bef_tax_y_on_y',
    n = sum( !is.na( inc_bef_tax_y_on_y ) ),
    mean = mean(inc_bef_tax_y_on_y),
    first_quartile = quantile(inc_bef_tax_y_on_y, 0.25),
    median   = median(inc_bef_tax_y_on_y),
    third_quartile = quantile(inc_bef_tax_y_on_y, 0.75),
    min = min(inc_bef_tax_y_on_y),
    max = max(inc_bef_tax_y_on_y),
    sd = sd(inc_bef_tax_y_on_y)
  )

# from the summary statistics, we should filter out extreme variables. Let's restrict max growth on profit at 200%, and min growth
# (max loss) at -200%

# check summary statistics again:
summary_EBIDTA_growth <- data %>% 
  filter(inc_bef_tax_y_on_y < 2 & inc_bef_tax_y_on_y > -2) %>% 
  summarise(
    Obs = 'inc_bef_tax_y_on_y',
    n = sum( !is.na( inc_bef_tax_y_on_y ) ),
    mean = mean(inc_bef_tax_y_on_y),
    first_quartile = quantile(inc_bef_tax_y_on_y, 0.25),
    median   = median(inc_bef_tax_y_on_y),
    third_quartile = quantile(inc_bef_tax_y_on_y, 0.75),
    min = min(inc_bef_tax_y_on_y),
    max = max(inc_bef_tax_y_on_y),
    sd = sd(inc_bef_tax_y_on_y)
  )

count(data, inc_bef_tax_y_on_y > 0)

# check distribution
data %>% 
  filter(inc_bef_tax_y_on_y < 2 & inc_bef_tax_y_on_y > -2) %>% 
  ggplot(aes(x = inc_bef_tax_y_on_y)) +
  geom_histogram(bins = 100)

# combine the 3 summary statistics together
growth_variables_summary <- summary_sales_growth %>% add_row( summary_profit_growth ) %>% add_row(summary_EBIDTA_growth)
growth_variables_summary
```

### 2.1 Label Engineering: 
The next step is to define our target variable, y, as fast-growth is not defined in the dataset. We consider several options to define a fast-growth firm. First, we decide to use 1-year growth, specifically from 2012 to 2013, to interpret and make more sense in economic terms. Next, we choose three financial variables, namely sales, profit/loss, and income before taxes, and apply this growth calculation. After cleaning and filtering out NAs and extreme variables, we report a combined summary statistic table below. It shows the total number of variables and, most importantly, the third quartile of each variable (We will talk about why the third quartile is essential later in the threshold decision phase). Sales growth gives 18909 variables, profit/loss gives 15156, and income before taxes gives 15145. We decide to pick sales growth based on the number of total variables. After confirming the growth variable, we need to create a fast-growth binary variable indicating firms who are a fast-growth with assigning them with 1. So how to pick the threshold is essential. We first try to compare them with the broader views of fast-growth firms: it is usually accepted that if a firm is growing faster than the overall economy, it should be considered fast-growing. In reality, the average European Union GDP growth for 2013 was at -0.055%. However, a substantial percentage of our data will be categorized into fast growth if using this number as the threshold. Therefore, we decide to use the third quartile of sales growth as the threshold. Any firm has a sales growth higher than 21.3% in 2013, and we will consider it a fast-growth firm. 

\begin{table}[htbp]
\centering
\caption{\bf Summary Statistics of Growth Variable Candidates}
\begin{tabular}[t]{lcccccccc}
\hline\hline
Obs & n & mean & First Quartile & Median & Third Quartile & Min & Max & SD\\
\hline
sales\_y\_on\_y & 18908 & -0.0182 & -0.293 & -0.00893 & 0.213 & -1 & 2 & 0.547 \\
\hline
profit\_loss\_year\_y\_on\_y & 15156 & -0.3285 & -0.957 & -0.43663 & 0.141 & -2 & 2 & 0.807 \\
\hline
inc\_bef\_tax\_y\_on\_y & 15145 & -0.2867 & -0.903 & -0.36816 & 0.184 & -2 & 2 & 0.810 \\
\hline
\end{tabular}
  \label{tab:shape-functions}
\end{table}


### 2.2 Sample Design 
For the sample data, we focus on the year 2012 alive firms. Like the case study, we decide to focus on the small and medium enterprise sector, captured by the firm's annual sales below 10 million euros. The reason is that fast-growth firms are usually small or medium start-ups with a lower number of sales initially, and we would like to focus on that in this study. We also decide to drop anything that is below 1000 euros. They seem to be firms that are not operating normally. As a result of our sample design, we include firms with annual sales below 10 million euros and exclude those are below 1000 euros in 2012. The total number of our cleaned sample size is 18248, and of which, 4609 of them will be fast growth in 2013. That is about 25% of the total firms. 


### 2.3 Feature Engineering  
In this process, we aim to select our predictive variables, clean and transform them into an appropriate form. We choose to use similar variables and features we used for exit prediction in the case study. Four groups are formed based on variables' attributes: size, management, financial variables and other characteristics. For the firm's characteristics, we include whether the firm has foreign management dummy, whether the firm's CEO is a female dummy, the location regions for its headquarters, age of the firm, industry categories and winsorized age and flags. Winsorized variables are generated to capture extreme values by establishing a threshold value and replacing values outside of this threshold by the threshold itself, and adding a flag variable. 

All the financial account variables should be non-negative, so we replace them with zero and add a binary variable to flag error. We also create ratios for profit/loss and balance sheet variables. The profit/loss variables are scaled by sales, whereas total assets scale balance sheet variables. 
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# We choose to use growth on sales as our growth vairbale since it gives the largest data sample and more consistant
data <- filter(data, sales_y_on_y < 2 & sales_y_on_y > -2)

# Binary variables for fast growth on SALES - target variable
data <- data %>% 
  mutate(
    fast_growth_sales = ifelse((sales_y_on_y >= 0.213), 1, 0))

# save file
write.csv(data, "Data/fast_growth_workingfile_targetchoosen.csv")


# generate status_alive; if sales larger than zero and not-NA, then firm is alive
data  <- data %>%
  mutate(status_alive = sales > 0 & !is.na(sales) %>%
           as.numeric(.))


#####################
### Sample design ##
####################
# data <- read_csv(file = "Data/Clean/fast_growth_workingfile_targetchoosen.csv")
data <- data %>%
  filter((year == 2012) & (status_alive == 1)) %>%
  # look at firms below 10m euro revenues and above 1000 euros
  filter(!(sales_mil > 10)) %>%
  filter(!(sales_mil < 0.001))


###########################################################
# Feature engineering
###########################################################

# change some industry category codes
data <- data %>%
  mutate(ind2_cat = ind2 %>%
           ifelse(. > 56, 60, .)  %>%
           ifelse(. < 26, 20, .) %>%
           ifelse(. < 55 & . > 35, 40, .) %>%
           ifelse(. == 31, 30, .) %>%
           ifelse(is.na(.), 99, .)
  )

table(data$ind2_cat)

# Firm characteristics
data <- data %>%
  mutate(age2 = age^2,
         foreign_management = as.numeric(foreign >= 0.5),
         gender_m = factor(gender, levels = c("female", "male", "mix")),
         m_region_loc = factor(region_m, levels = c("Central", "East", "West")))

###########################################################
# look at more financial variables, create ratios
###########################################################

# assets can't be negative. Change them to 0 and add a flag.
data <-data  %>%
  mutate(flag_asset_problem=ifelse(intang_assets<0 | curr_assets<0 | fixed_assets<0,1,0  ))
table(data$flag_asset_problem)

data <- data %>%
  mutate(intang_assets = ifelse(intang_assets < 0, 0, intang_assets),
         curr_assets = ifelse(curr_assets < 0, 0, curr_assets),
         fixed_assets = ifelse(fixed_assets < 0, 0, fixed_assets))

# generate total assets
data <- data %>%
  mutate(total_assets_bs = intang_assets + curr_assets + fixed_assets)
summary(data$total_assets_bs)


pl_names <- c("extra_exp","extra_inc",  "extra_profit_loss", "inventories",
              "material_exp", "personnel_exp", "inc_bef_tax", "profit_loss_year")
bs_names <- c("intang_assets", "curr_liab", "fixed_assets", "liq_assets", "curr_assets",
              "share_eq", "subscribed_cap", "tang_assets" )

# Create rations
# divide all pl_names elements by sales and create new column for it
data <- data %>%
  mutate_at(vars(pl_names), funs("pl"=./sales))

# divide all bs_names elements by total_assets_bs and create new column for it
data <- data %>%
  mutate_at(vars(bs_names), funs("bs"=ifelse(total_assets_bs == 0, 0, ./total_assets_bs)))


########################################################################
# creating flags, and winsorizing tails
########################################################################

# Variables that represent accounting items that cannot be negative (e.g. materials)
zero <-  c("extra_exp_pl", "extra_inc_pl", "inventories_pl", "material_exp_pl", "personnel_exp_pl",
           "curr_liab_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs", "subscribed_cap_bs",
           "intang_assets_bs")

data <- data %>%
  mutate_at(vars(zero), funs("flag_high"= as.numeric(.> 1))) %>%
  mutate_at(vars(zero), funs(ifelse(.> 1, 1, .))) %>%
  mutate_at(vars(zero), funs("flag_error"= as.numeric(.< 0))) %>%
  mutate_at(vars(zero), funs(ifelse(.< 0, 0, .)))


# for vars that could be any, but are mostly between -1 and 1
any <-  c("extra_profit_loss_pl", "share_eq_bs", "inc_bef_tax_pl", "profit_loss_year_pl")

data <- data %>%
  mutate_at(vars(any), funs("flag_low"= as.numeric(.< -1))) %>%
  mutate_at(vars(any), funs(ifelse(.< -1, -1, .))) %>%
  mutate_at(vars(any), funs("flag_high"= as.numeric(.> 1))) %>%
  mutate_at(vars(any), funs(ifelse(.> 1, 1, .))) %>%
  mutate_at(vars(any), funs("flag_zero"= as.numeric(.== 0))) %>%
  mutate_at(vars(any), funs("quad"= .^2))


# dropping flags with no variation
variances<- data %>%
  select(contains("flag")) %>%
  apply(2, var, na.rm = TRUE) == 0

data <- data %>%
  select(-one_of(names(variances)[variances]))

########################################################################
# additional
# including some imputation
########################################################################

# CEO age
data <- data %>%
  mutate(ceo_age = year-birth_year,
         flag_low_ceo_age = as.numeric(ceo_age < 25 & !is.na(ceo_age)),
         flag_high_ceo_age = as.numeric(ceo_age > 75 & !is.na(ceo_age)),
         flag_miss_ceo_age = as.numeric(is.na(ceo_age)))

data <- data %>%
  mutate(ceo_age = ifelse(ceo_age < 25, 25, ceo_age) %>%
           ifelse(. > 75, 75, .) %>%
           ifelse(is.na(.), mean(., na.rm = TRUE), .),
         ceo_young = as.numeric(ceo_age < 40))

# number emp, very noisy measure
data <- data %>%
  mutate(labor_avg_mod = ifelse(is.na(labor_avg), mean(labor_avg, na.rm = TRUE), labor_avg),
         flag_miss_labor_avg = as.numeric(is.na(labor_avg)))

summary(data$labor_avg)
summary(data$labor_avg_mod)

data <- data %>%
  select(-labor_avg)

# create factors
data <- data %>%
  mutate(urban_m = factor(urban_m, levels = c(1,2,3)),
         ind2_cat = factor(ind2_cat, levels = sort(unique(data$ind2_cat))))

data <- data %>%
  mutate(fast_growth_sales_f = factor(fast_growth_sales, levels = c(0,1)) %>%
           recode(., `0` = 'no_fast_growth', `1` = "fast_growth"))


# generate variables ---------------------------------------------------
data <- data %>%
  mutate(sales_mil_log_sq=sales_mil_log^2)

data <- data %>%
  mutate(flag_low_d1_sales_mil_log = ifelse(d1_sales_mil_log < -1.5, 1, 0),
         flag_high_d1_sales_mil_log = ifelse(d1_sales_mil_log > 1.5, 1, 0),
         d1_sales_mil_log_mod = ifelse(d1_sales_mil_log < -1.5, -1.5,
                                       ifelse(d1_sales_mil_log > 1.5, 1.5, d1_sales_mil_log)),
         d1_sales_mil_log_mod_sq = d1_sales_mil_log_mod^2
  )

# no more imputation, drop obs if key vars missing
data <- data %>%
  filter(!is.na(liq_assets_bs),!is.na(foreign), !is.na(ind))

# drop missing
data <- data %>%
  filter(!is.na(age),!is.na(foreign), !is.na(material_exp_pl), !is.na(m_region_loc))
# Hmisc::describe(data$age)

# drop unused factor levels
data <- data %>%
  mutate_at(vars(colnames(data)[sapply(data, is.factor)]), funs(fct_drop))


```


## 3. Probability Prediction and Model Selection: Logit Models 
This section will carry out a probability prediction by logit, select the best model by cross-validation, and evaluate the holdout set's best prediction. We take 80% of our data as a working sample, which has 12936 observations, and the rest 20% as our holdout set, which has 3234 observations. 

 

### 3.1 Categorizing the variables 
The first step of building models is to separate the different variables into categories to know when to add which ones. This is done to determine what type of variables have the most considerable effect on the dependent variable. In this case, first, the raw variables have been separated from the modified ones. The ratio variables have also been split from the raw ones since they show a different aspect. These emphasize how each is in terms of the whole (total assets or total balance sheet). This is a crucial aspect since the size of the number does not in itself say anything. It could be large or small, but that will only show in comparison with the whole. Second, the squared variables were separated, their manipulated state would have a very different effect when combined with the raw variables. The flags have been put in their category to be able to show their effect separately. Sometimes knowing is too high or low can be interesting, while this fact is almost irrelevant. 

A growth category (d1) was created since this is especially important to our study of fast-growing firms. The last two categories created were HR and Firm, which show different aspects of a company. HR includes gender ratio, age of CEO, and other internal factors related to the employees. On the other hand, Firm includes more fundamental factors related to the company itself, such as age, location, etc. Two categories of interactions were also created to feed a logit LASSO model as many variables as possible. One of these was based on numerical variables and the other on non-numeric. This split between the variables allows for the building of models with different focuses.   

 
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
#######################################################################################################################
# Prediction Part
#######################################################################################################################

# Define variable sets ----------------------------------------------
# (making sure we use ind2_cat, which is a factor)

rawvars <-  c("curr_assets", "curr_liab", "extra_exp", "extra_inc", "extra_profit_loss", "fixed_assets",
              "inc_bef_tax", "intang_assets", "inventories", "liq_assets", "material_exp", "personnel_exp",
              "profit_loss_year", "sales", "share_eq", "subscribed_cap")
qualityvars <- c("balsheet_flag", "balsheet_length", "balsheet_notfullyear")
engvar <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs",
            "share_eq_bs", "subscribed_cap_bs", "intang_assets_bs", "extra_exp_pl",
            "extra_inc_pl", "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
            "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl")
engvar2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
             "profit_loss_year_pl_quad", "share_eq_bs_quad")
engvar3 <- c(grep("*flag_low$", names(data), value = TRUE),
             grep("*flag_high$", names(data), value = TRUE),
             grep("*flag_error$", names(data), value = TRUE),
             grep("*flag_zero$", names(data), value = TRUE))
d1 <-  c("d1_sales_mil_log_mod", "d1_sales_mil_log_mod_sq",
         "flag_low_d1_sales_mil_log", "flag_high_d1_sales_mil_log")
hr <- c("female", "ceo_age", "flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")
firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")

# interactions for logit, LASSO
interactions1 <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*d1_sales_mil_log_mod", "ind2_cat*sales_mil_log",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", "ind2_cat*labor_avg_mod")
interactions2 <- c("sales_mil_log*age", "sales_mil_log*female",
                   "sales_mil_log*profit_loss_year_pl", "sales_mil_log*foreign_management")


X1 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat")
X2 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "fixed_assets_bs","share_eq_bs","curr_liab_bs ",   "curr_liab_bs_flag_high ", "curr_liab_bs_flag_error",  "age","foreign_management" , "ind2_cat")
X3 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, d1)
X4 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr, qualityvars)
X5 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr, qualityvars, interactions1, interactions2)

# for LASSO
logitvars <- c("sales_mil_log", "sales_mil_log_sq", engvar, engvar2, engvar3, d1, hr, firm, qualityvars, interactions1, interactions2)

# for RF (no interactions, no modified features)
rfvars  <-  c("sales_mil", "d1_sales_mil_log", rawvars, hr, firm, qualityvars)


# separate datasets -------------------------------------------------------

set.seed(13505)

train_indices <- as.integer(createDataPartition(data$fast_growth_sales, p = 0.8, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

dim(data_train)
dim(data_holdout)

Hmisc::describe(data$fast_growth_sales_f)
Hmisc::describe(data_train$fast_growth_sales_f)
Hmisc::describe(data_holdout
                $fast_growth_sales_f)

#######################################################x
# PART I PREDICT PROBABILITIES
# Predict logit models ----------------------------------------------
#######################################################x

# define function
twoClassSummaryExtended <- function (data, lev = NULL, model = NULL)
{
  lvls <- levels(data$obs)
  rmse <- sqrt(mean((data[, lvls[1]] - ifelse(data$obs == lev[2], 0, 1))^2))
  c(defaultSummary(data, lev, model), "RMSE" = rmse)
}


# 5 fold cross-validation
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)


# Train Logit Models ----------------------------------------------

logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)

CV_RMSE_folds <- list()
logit_models <- list()

for (model_name in names(logit_model_vars)) {
  
  features <- logit_model_vars[[model_name]]
  
  set.seed(13505)
  glm_model <- train(
    formula(paste0("fast_growth_sales_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )
  
  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]
  
}

# Logit lasso -----------------------------------------------------------

lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(13505)
system.time({
  logit_lasso_model <- train(
    formula(paste0("fast_growth_sales_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )
})

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
# save file
write.csv(lasso_coeffs, "Data/lasso_logit_coeffs.csv")

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]




#############################################x
# PART I
# No loss fn
########################################

# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {
  
  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }
  
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                           "AUC" = unlist(auc))
}

# For each model: average RMSE and average AUC for models ----------------------------------

CV_RMSE <- list()
CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

# We have 6 models, (5 logit and the logit lasso). For each we have a 5-CV RMSE and AUC.
# We pick our preferred model based on that. -----------------------------------------------

nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)

logit_summary1 <- data.frame("Number of predictors" = unlist(nvars),
                             "CV RMSE" = unlist(CV_RMSE),
                             "CV AUC" = unlist(CV_AUC))


# Take best model and estimate RMSE on holdout  -------------------------------------------

best_logit_no_loss <- logit_models[["LASSO"]]

logit_predicted_probabilities_holdout <- predict(best_logit_no_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_no_loss_pred"] <- logit_predicted_probabilities_holdout[,"fast_growth"]
RMSE(data_holdout[, "best_logit_no_loss_pred", drop=TRUE], data_holdout$fast_growth_sales)

# discrete ROC (with thresholds in steps) on holdout -------------------------------------------------
thresholds <- seq(0.05, 0.75, by = 0.05)

cm <- list()
true_positive_rates <- c()
false_positive_rates <- c()
for (thr in thresholds) {
  holdout_prediction <- ifelse(data_holdout[,"best_logit_no_loss_pred"] < thr, "no_fast_growth", "fast_growth") %>%
    factor(levels = c("no_fast_growth", "fast_growth"))
  cm_thr <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_sales_f)$table
  cm[[as.character(thr)]] <- cm_thr
  true_positive_rates <- c(true_positive_rates, cm_thr["fast_growth", "fast_growth"] /
                             (cm_thr["fast_growth", "fast_growth"] + cm_thr["no_fast_growth", "fast_growth"]))
  false_positive_rates <- c(false_positive_rates, cm_thr["fast_growth", "no_fast_growth"] /
                              (cm_thr["fast_growth", "no_fast_growth"] + cm_thr["no_fast_growth", "no_fast_growth"]))
}

tpr_fpr_for_thresholds <- tibble(
  "threshold" = thresholds,
  "true_positive_rate" = true_positive_rates,
  "false_positive_rate" = false_positive_rates
)

library(viridis)
```

### 3.2 Probability Prediction 
First, we consider five logit models and LASSO. The five logit models differ from predictor variables and their interactions. The basic model includes sales, sales growth, profit/loss and industry categories. We then add firms' financial variables and attributes, followed by growth, HR, and data quality. Finally, for the last logit model and the logit LASSO model, we include 13 interactions, from the firm's age to industry categories. As we said earlier, we take 80% of our data as a work set for our model's predictive performance, and the rest 20% random sample as a holdout set. We then use a 5-fold-cross-validation to divide the work set into training and test sets five times. We evaluate each model by its average RMSE. The six model results are shown below. Logit LASSO starts with the logit M5 model that includes all the variables and interactions and shrinks the coefficients from 151 to 47 to select variables for a better predictive model. It gives the lowest average RMSE of 0.424. Logit M4 has a similar RMSE but presents with a more complicated model with a higher number of coefficients. As a result, we consider Logit LASSO as our benchmark model within the logit models. After estimating the model on all observations in the work sample, we calculate its RMSE in the random holdout sample. The RMSE for LASSO is 0.417, which is smaller than the cross-validated RMSE. 


\begin{table}[htbp]
\caption{\bf Cross-validated RMSE of Logit Models}
\centering
\begin{tabular}{lccc}
\hline\hline
  & N Vars & N coeffs & CV RMSE\\
\hline
Logit M1 & 4 & 12 & 0.431\\
Logit M2 & 9 & 19 & 0.428\\
Logit M3 & 22 & 36 & 0.426\\
Logit M4 & 30 & 77 & 0.425\\
Logit M5 & 30 & 151 & 0.426\\
LASSO & 30 & 47 & 0.424\\
\hline
\end{tabular}
  \label{tab:shape-functions}
\end{table}



### 3.3 ROC Curve and Classification Diagnostics by the Confusion Table
Based on the chosen Logit LASSO model, we would like to see what types of mistakes we make when using a different set of threshold values. Specifically, we look at a set of threshold values between 0.05 to 0.75, by steps of 0.05. The below two figures are ROC curves for predicting high-growth firms. 

The colours on the left figure correspond to the threshold values' level. The lighter the color, the lower the threshold level. As a result, we can see that if our threshold is substantial (in the bottom left corner), there are very few cases that are predicted to be positive. Similarly, the true positive rate and the false positive rate are also low. Moreover, as we lower the threshold value, more and more positive cases will happen according to the ROC curve. This is good on one hand because a higher true-positive rate means higher accuracy. However, a higher false-positive rate is alarming because it indicates we are making more mistakes. If we are willing to make more mistakes when we accept more errors, we can set the threshold low and have a high true-positive rate at the expense of actually having a high rate of making mistakes. If itâ€™s too costly to have positive errors, then we should increase the threshold, and accept the fact that our true-positive rate is going to be low.

The right-hand side graph is similar to the one on the left, but it is continuous and has no reference to the corresponding threshold values. The 45-degree line represents an entirely random prediction, so the further above the 45-degree line, the better the prediction would be.

```{r, message = F, echo = FALSE, warning=FALSE, fig.height= 3, fig.width= 8,  fig.align='center'}
discrete_roc_plot <- ggplot(
  data = tpr_fpr_for_thresholds,
  aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  labs(x = "False positive rate (1 - Specificity)", y = "True positive rate (Sensitivity)") +
  geom_point(size=2, alpha=0.8) +
  scale_color_viridis(option = "D", direction = -1) +
  scale_x_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  theme_light() +
  theme(axis.text.x=element_text(size=6,face = "plain")) +
  theme(axis.text.y=element_text(size=6,face = "plain")) +
  theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
  theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain")) +
  theme(legend.position ="right") +
  theme(legend.title = element_text(size = 4), 
        legend.text = element_text(size = 4),
        legend.key.size = unit(.4, "cm")) 


# continuous ROC on holdout with best model (Logit 4) -------------------------------------------

roc_obj_holdout <- roc(data_holdout$fast_growth_sales, data_holdout$best_logit_no_loss_pred)

createRocPlot <- function(r, file_name,  myheight_small = 5.625, mywidth_small = 7.5) {
  all_coords <- coords(r, x="all", ret="all", transpose = FALSE)
  
  roc_plot <- ggplot(data = all_coords, aes(x = fpr, y = tpr)) +
    geom_line(color='darkgreen', size = 0.7) +
    geom_area(aes(fill = 'orange', alpha=0.4), alpha = 0.3, position = 'identity', color = 'darkgreen') +
    scale_fill_viridis(discrete = TRUE, begin=0.6, alpha=0.5, guide = FALSE) +
    xlab("False Positive Rate (1-Specifity)") +
    ylab("True Positive Rate (Sensitivity)") +
    geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0, 0.01)) +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0.01, 0)) +
    theme_light() +
    theme(axis.text.x=element_text(size=6,face = "plain")) +
    theme(axis.text.y=element_text(size=6,face = "plain")) +
    theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
    theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain"))
  #+    theme(axis.text.x = element_text(size=13), axis.text.y = element_text(size=13),
  #        axis.title.x = element_text(size=13), axis.title.y = element_text(size=13))
  #save_fig(file_name, output, "small")
  
  #ggsave(plot = roc_plot, paste0(file_name, ".png"),      width=mywidth_small, height=myheight_small, dpi=1200)
  #cairo_ps(filename = paste0(file_name, ".eps"),    #        width = mywidth_small, height = myheight_small, pointsize = 12,    #       fallback_resolution = 1200)
  #print(roc_plot)
  #dev.off()
  
  roc_plot
}

grid.arrange(discrete_roc_plot,createRocPlot(roc_obj_holdout, "best_logit_no_loss_roc_plot_holdout"), ncol = 2)
```

The shaded area below the ROC curve in the right-side figure is the AUC (area under the curve). The below table shows the 5-fold cross-validation RMSE and the average AUC. The logit LASSO modal has the lowest RMSE but not necessarily the highest AUC. 

\begin{table}[htbp]
\caption{\bf RMSE and AUC for Various Models of High-growth Firms}
\centering
\begin{tabular}{lccc}
\hline\hline
  & Number of predictors & CV RMSE & CV AUC\\
\hline
Logit M1 & 11 & 0.431 & 0.574\\
Logit M2 & 18 & 0.428 & 0.605\\
Logit M3 & 35 & 0.426 & 0.622\\
Logit M4 & 76 & 0.425 & 0.627\\
Logit M5 & 150 & 0.426 & 0.629\\
LASSO & 46 & 0.424 & 0.614\\
\hline
\end{tabular}
  \label{tab:shape-functions}
\end{table}


Once we pick the best model for predicting probabilities, we illustrate classification using the holdout set. We show the confusion table for two possible thresholds: 0.5 and 0.25. 0.5 is the default choice of R if no threshold is specified, and 0.25 is picked based on the percentage of high-growth firms selected from the data. The below table shows the confusion tables for both of these thresholds. We can see that we have a lower true-positive rate from the confusion table when having a higher threshold, 2% with threshold 0.5, and 14% with a threshold of 0.25. The same goes for false positive, 1% with a threshold of 0.5, and 30% with a threshold of 0.25.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# fast_growth: the threshold 0.5 is used to convert probabilities to binary classes
logit_class_prediction <- predict(best_logit_no_loss, newdata = data_holdout)
summary(logit_class_prediction)

# confusion matrix: summarize different type of errors and successfully predicted cases
# positive = "yes": explicitly specify the positive case
cm_object1 <- confusionMatrix(logit_class_prediction, data_holdout$fast_growth_sales_f, positive = "fast_growth")
cm1 <- cm_object1$table
cm1

# we can apply different thresholds

# 0.5 same as before
holdout_prediction <-
  ifelse(data_holdout$best_logit_no_loss_pred < 0.5, "no_fast_growth", "fast_growth") %>%
  factor(levels = c("no_fast_growth", "fast_growth"))
cm_object1b <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_sales_f)
cm1b <- cm_object1b$table
cm1b

# a sensible choice: mean of predicted probabilities
mean_predicted_default_prob <- mean(data_holdout$best_logit_no_loss_pred)
mean_predicted_default_prob
holdout_prediction <-
  ifelse(data_holdout$best_logit_no_loss_pred < mean_predicted_default_prob, "no_fast_growth", "fast_growth") %>%
  factor(levels = c("no_fast_growth", "fast_growth"))
cm_object2 <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_sales_f)
cm2 <- cm_object2$table
cm2
```

Table 5 shows that the 0.5 threshold leads to a higher accuracy rate than 0.25 and a higher specificity rate. At the same time, the 0.25 threshold results in higher sensitivity.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lccc|ccc}
         &\multicolumn{3}{c|}{Threshold: 0.5} & \multicolumn{3}{c}{Threshold: 0.25}  \\
         \hline
         & Actual No FG & FG & Total &  Actual No FG & FG & Total \\
         \hline
         Predicted No FG & 75\% & 22\% & 97\% & 46\% & 10\% & 56\% \\
         Predicted FG & 1\% & 2\% & 3\% & 30\% & 14\% & 44\% \\
         Total & 76\% & 24\% & 100\% & 76\% & 24\% & 100\% \\
         \hline
    \end{tabular}
    \caption{\bf Confusion Tables with Thresholds 0.5 and 0.25}
    \label{tab:my_label}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\hline\hline
  & Threshold: 0.5 & Threshold: 0.25 \\
\hline
Accuracy & 77\% & 60\%\\
Sensitivity & 8\% & 58\%\\
Specificity & 99\% & 61\%\\
\hline
\end{tabular}
  \caption{\bf Accuracy, Sensitivity, and Specificity for Threshold values 0.5 and 0.25 (in percentage)}
  \label{tab:shape-functions}
\end{table}



## 4. Classification from Predicted Probabilities: Logit Models 
With the help of a loss function, we can find the threshold that gets us the lowest expected loss. In our case, the "positive" event is a fast growth, leading to generating high revenue and paying out full debts. The "negative" event is not fast growth, which would probably generate a low revenue and not pay out full debts. If we think about this from a bank's perspective: a false positive would be we predict the firm would be fast-growing and would be able to pay back full in the subsequential year, so we accept the business, but in reality, it is not fast growth and not able to pay back full. On the other hand, a false negative would be we predict that the firm would not be fast-growing and would not be able to pay back full, so we reject the business, but in reality, it is a fast-growing firm and generating high revenue. Both of them are a loss for the bank, but the latter (false negative) is an opportunity loss, which is better than a false positive with actual loss from the business. As a result, we decide to assign the opportunity loss (false negative) to 1 thousand euro and false positive 5 thousand euro. The below shows the value of the loss function.

\begin{table}[htbp]
\centering
\begin{tabular}{lcc}
\hline\hline
  & Actual negative & Actual Positive \\
\hline
Predicted negative & 0 & 1\\
Predicted positive & 10 & 0\\
\hline
\end{tabular}
  \caption{\bf Loss Function for Classification}
  \label{tab:shape-functions}
\end{table}

As a result, given that our FP loss is 5, FN loss is 1, the optimal threshold is 1/6 = 0.167. We also plot the expected loss with all possible threshold values from 0 to 1 and the lowest expected loss selected by the algorithm for one test set. The selected threshold is 0.19 with an expected loss of 0.73 with the Logit LASSO model.

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Introduce loss function
# relative cost of of a false negative classification (as compared with a false positive classification)
FP=1 
FN=5 
cost= FN/FP 

# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevelance = sum(data_train$fast_growth_sales)/length(data_train$fast_growth_sales)

# Draw ROC Curve and find optimal threshold with loss function --------------------------

best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {
  
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  
  best_tresholds_cv <- list()
  expected_loss_cv <- list()
  
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevelance))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$fast_growth)
  }
  
  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv))
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv))
  
  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]
  
}

logit_summary2 <- data.frame("Avg of optimal thresholds" = unlist(best_tresholds),
                             "Threshold for Fold5" = sapply(logit_cv_threshold, function(x) {x$threshold}),
                             "Avg expected loss" = unlist(expected_loss),
                             "Expected loss for Fold5" = unlist(logit_cv_expected_loss))

kable(x = logit_summary2, format = "latex", booktabs=TRUE,  digits = 3, row.names = TRUE,
      linesep = "", col.names = c("Avg of optimal thresholds","Threshold for fold #5",
                                  "Avg expected loss","Expected loss for fold #5")) %>%
  cat(.,file= paste0('/Users/xinqi/Desktop/DA3/Assignment_2/', "logit_summary1.tex"))

# Create plots based on Fold5 in CV ----------------------------------------------
createLossPlot <- function(r, best_coords, file_name,  myheight_small = 5.625, mywidth_small = 7.5) {
  t <- best_coords$threshold[1]
  sp <- best_coords$specificity[1]
  se <- best_coords$sensitivity[1]
  n <- rowSums(best_coords[c("tn", "tp", "fn", "fp")])[1]
  
  all_coords <- coords(r, x="all", ret="all", transpose = FALSE)
  all_coords <- all_coords %>%
    mutate(loss = (fp*FP + fn*FN)/n)
  l <- all_coords[all_coords$threshold == t, "loss"]
  
  loss_plot <- ggplot(data = all_coords, aes(x = threshold, y = loss)) +
    geom_line(color='orange', size=0.7) +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.1)) +
    geom_vline(xintercept = t , color = 'darkgreen' ) +
    annotate(geom = "text", x = t, y= min(all_coords$loss),
             label=paste0("best threshold: ", round(t,2)),
             colour='darkgreen', angle=90, vjust = -1, hjust = -0.5, size = 7) +
    annotate(geom = "text", x = t, y= l,
             label= round(l, 2), hjust = -0.3, size = 7) +
    theme_light() +
    theme(axis.text.x=element_text(size=6,face = "plain")) +
    theme(axis.text.y=element_text(size=6,face = "plain")) +
    theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
    theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain"))
  # save_fig(file_name, output, "small")
  
  #  ggsave(plot = loss_plot, paste0(file_name,".png"), width=mywidth_small, height=myheight_small, dpi=1200)
  #  cairo_ps(filename = paste0(file_name,".eps"), width = mywidth_small, height = myheight_small, pointsize = 12, fallback_resolution = 1200)
  #  print(loss_plot)
  #  dev.off()
  
  loss_plot
}

createRocPlotWithOptimal <- function(r, best_coords, file_name,  myheight_small = 5.625, mywidth_small = 7.5) {
  
  all_coords <- coords(r, x="all", ret="all", transpose = FALSE)
  t <- best_coords$threshold[1]
  sp <- best_coords$specificity[1]
  se <- best_coords$sensitivity[1]
  
  roc_plot <- ggplot(data = all_coords, aes(x = specificity, y = sensitivity)) +
    geom_line(color='orange', size=0.7) +
    scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +
    scale_x_reverse(breaks = seq(0, 1, by = 0.1)) +
    geom_point(aes(x = sp, y = se)) +
    annotate(geom = "text", x = sp, y = se,
             label = paste(round(sp, 2),round(se, 2),sep = ", "),
             hjust = 1, vjust = -1, size = 7) +
    theme_light() +
    theme(axis.text.x=element_text(size=6,face = "plain")) +
    theme(axis.text.y=element_text(size=6,face = "plain")) +
    theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
    theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain"))
  #  + theme(axis.text.x = element_text(size=20), axis.text.y = element_text(size=20),
  #          axis.title.x = element_text(size=20), axis.title.y = element_text(size=20))
  #save_fig(file_name, output, "small")
  
  #  ggsave(plot = roc_plot, paste0(file_name, ".png"),         width=mywidth_small, height=myheight_small, dpi=1200)
  # cairo_ps(filename = paste0(file_name, ".eps"),           width = mywidth_small, height = myheight_small, pointsize = 12,           fallback_resolution = 1200)
  #print(roc_plot)
  #dev.off()
  
  roc_plot
}

for (model_name in names(logit_cv_rocs)) {
  
  r <- logit_cv_rocs[[model_name]]
  best_coords <- logit_cv_threshold[[model_name]]
  createLossPlot(r, best_coords,
                 paste0(model_name, "_loss_plot"))
  createRocPlotWithOptimal(r, best_coords,
                           paste0(model_name, "_roc_plot"))
}

```


```{r, message = F, echo = FALSE, warning=FALSE, fig.align='center', out.width = '50%'}
createLossPlot(r, best_coords,
                 paste0(model_name, "_loss_plot"))
```

Table 7 summarizes our results. The optimal classification threshold turns out to be quite similar, except for logit M5. The model that has the smallest RMSE (logit LASSO) turns out not to be the one that has the smallest expected loss. Logit M3 turns out does.

\begin{table}[htbp]
\centering
\begin{tabular}{lccc}
\hline\hline
 & RMSE & Optimal Threshold & Expected Loss \\ 
  \hline
Logit M1 & 0.431 & 0.171 & 0.735 \\ 
Logit M2 & 0.428 & 0.172 & 0.728 \\ 
Logit M3 & 0.426 & 0.165 & 0.712 \\ 
Logit M4 & 0.425 & 0.171 & 0.717 \\ 
Logit M5 & 0.426 & 0.141 & 0.715 \\ 
Logit LASSO & 0.424 & 0.169 & 0.730 \\ 
\hline\hline
\end{tabular}
   \caption{\bf Cross-validated Ranking of Classifications}
\end{table}

```{r, echo=FALSE, message=FALSE, warning=FALSE, results='hide', fig.show='hide'}
# Pick best model based on average expected loss ----------------------------------

best_logit_with_loss <- logit_models[["X3"]]
best_logit_optimal_treshold <- best_tresholds[["X3"]]

logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"fast_growth"]

# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$fast_growth_sales, data_holdout[, "best_logit_with_loss_pred", drop=TRUE])

# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$fast_growth_sales)
expected_loss_holdout

# Confusion table on holdout with optimal threshold
holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_fast_growth", "fast_growth") %>%
  factor(levels = c("no_fast_growth", "fast_growth"))
cm_object3 <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_sales_f)
cm3 <- cm_object3$table
cm3
```












## Appendix
```{r, message = F, echo = FALSE, warning=FALSE, fig.height= 3, fig.width= 8,  fig.align='center'}
########################################################################
# sales CHECK
########################################################################

sales_sum_plot <- ggplot(data = data, aes(x=sales_mil_log, y=as.numeric(fast_growth_sales))) +
  geom_point(size=2,  shape=20, stroke=2, fill="blue", color="blue") +
  geom_smooth(method = "lm", formula = y ~ poly(x,2), color='orange', se = F, size=1)+
  geom_smooth(method="loess", se=F, colour='darkgreen', size=1.5, span=0.9) +
  labs(x = "sales_mil_log",y = "fast_growth_firms") +
  theme_light() +
  theme(axis.text.x=element_text(size=6,face = "plain")) +
  theme(axis.text.y=element_text(size=6,face = "plain")) +
  theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
  theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain"))


ols_s <- lm(fast_growth_sales~sales_mil_log+sales_mil_log_sq,
            data = data)
# summary(ols_s)

# lowess
# Hmisc::describe(data$d1_sales_mil_log) # no missing

d1sale_1<-ggplot(data = data, aes(x=d1_sales_mil_log, y=as.numeric(fast_growth_sales))) +
  geom_point(size=0.1,  shape=20, stroke=2, fill='orange', color='orange') +
  geom_smooth(method="loess", se=F, colour='darkgreen', size=1.5, span=0.9) +
  labs(x = "Growth rate (Diff of ln sales)",y = "fast_growth_firms") +
  theme_light() +
  theme(axis.text.x=element_text(size=6,face = "plain")) +
  theme(axis.text.y=element_text(size=6,face = "plain")) +
  theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
  theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain")) +
  scale_x_continuous(limits = c(-6,10), breaks = seq(-5,10, 5))

grid.arrange(sales_sum_plot,d1sale_1, ncol = 2)

d1sale_2<-ggplot(data = data, aes(x=d1_sales_mil_log_mod, y=as.numeric(fast_growth_sales))) +
  geom_point(size=0.1,  shape=20, stroke=2, fill='orange', color='orange') +
  geom_smooth(method="loess", se=F, colour='darkgreen', size=1.5, span=0.9) +
  labs(x = "Growth rate (Diff of ln sales)",y = "fast_growth_firms") +
  theme_light() +
  theme(axis.text.x=element_text(size=6,face = "plain")) +
  theme(axis.text.y=element_text(size=6,face = "plain")) +
  theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
  theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain")) +
  scale_x_continuous(limits = c(-1.5,1.5), breaks = seq(-1.5,1.5, 0.5))

d1sale_3<-ggplot(data = data, aes(x=d1_sales_mil_log, y=d1_sales_mil_log_mod)) +
  geom_point(size=0.1,  shape=20, stroke=2, fill='orange', color='orange') +
  labs(x = "Growth rate (Diff of ln sales) (original)",y = "Growth rate (Diff of ln sales) (winsorized)") +
  theme_light() +
  theme(axis.text.x=element_text(size=6,face = "plain")) +
  theme(axis.text.y=element_text(size=6,face = "plain")) +
  theme(axis.title.x=element_text(size=6, vjust=0, face = "plain")) +
  theme(axis.title.y=element_text(size=6,vjust=1.25, face = "plain")) +
  scale_x_continuous(limits = c(-5,5), breaks = seq(-5,5, 1)) +
  scale_y_continuous(limits = c(-3,3), breaks = seq(-3,3, 1))

grid.arrange(d1sale_2,d1sale_3, ncol = 2)
```




